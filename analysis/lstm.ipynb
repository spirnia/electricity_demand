{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta,datetime\n",
    "df=pd.read_csv('C:\\\\workplace\\\\electricity_demand\\\\dataset\\\\AEP_hourly.csv')\n",
    "df.columns=['date_time','aep_mw']\n",
    "df['date_time']=pd.to_datetime(df['date_time'])\n",
    "df.sort_values('date_time',ascending=True,inplace=True)\n",
    "df['date_time']=df['date_time'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparion_train (df,num_lags,lag_feature):\n",
    "\n",
    "    df_lags=pd.DataFrame(index=df.index,columns=['lags_{}'.format(671-i) for i in range(num_lags)])\n",
    "    for i in range(num_lags):\n",
    "        df_lags['lags_{}'.format(671-i)]=df[lag_feature].shift(671-i)\n",
    "    \n",
    "    df_targets=pd.DataFrame(index=df.index,columns=['step_{}'.format(i) for i in range(16,40)])\n",
    "    for i in range(16,40):\n",
    "        df_targets['step_{}'.format(i)]=df[lag_feature].shift(-i)\n",
    "    \n",
    "    df_processed=pd.concat([df[['date_time']],df_targets,df_lags],axis=1)\n",
    "    #df_processed=df_processed[pd.to_datetime(df_processed['date_time']).dt.hour==trigerred_time]\n",
    "    return df_processed.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_time</th>\n",
       "      <th>aep_mw</th>\n",
       "      <th>scaled_aep_mw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2183</th>\n",
       "      <td>2004-10-01 01:00:00</td>\n",
       "      <td>12379.0</td>\n",
       "      <td>-1.276695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184</th>\n",
       "      <td>2004-10-01 02:00:00</td>\n",
       "      <td>11935.0</td>\n",
       "      <td>-1.492740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185</th>\n",
       "      <td>2004-10-01 03:00:00</td>\n",
       "      <td>11692.0</td>\n",
       "      <td>-1.614218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2186</th>\n",
       "      <td>2004-10-01 04:00:00</td>\n",
       "      <td>11597.0</td>\n",
       "      <td>-1.662359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2187</th>\n",
       "      <td>2004-10-01 05:00:00</td>\n",
       "      <td>11681.0</td>\n",
       "      <td>-1.619773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date_time   aep_mw  scaled_aep_mw\n",
       "2183  2004-10-01 01:00:00  12379.0      -1.276695\n",
       "2184  2004-10-01 02:00:00  11935.0      -1.492740\n",
       "2185  2004-10-01 03:00:00  11692.0      -1.614218\n",
       "2186  2004-10-01 04:00:00  11597.0      -1.662359\n",
       "2187  2004-10-01 05:00:00  11681.0      -1.619773"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "scaler=PowerTransformer().fit(df.loc[df['date_time']<='2017-08-02 23:00:00',['aep_mw']])\n",
    "df['scaled_aep_mw']=pd.Series(scaler.transform(df[['aep_mw']]).reshape(-1),index=df.index)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_time</th>\n",
       "      <th>step_16</th>\n",
       "      <th>step_17</th>\n",
       "      <th>step_18</th>\n",
       "      <th>step_19</th>\n",
       "      <th>step_20</th>\n",
       "      <th>step_21</th>\n",
       "      <th>step_22</th>\n",
       "      <th>step_23</th>\n",
       "      <th>step_24</th>\n",
       "      <th>...</th>\n",
       "      <th>lags_9</th>\n",
       "      <th>lags_8</th>\n",
       "      <th>lags_7</th>\n",
       "      <th>lags_6</th>\n",
       "      <th>lags_5</th>\n",
       "      <th>lags_4</th>\n",
       "      <th>lags_3</th>\n",
       "      <th>lags_2</th>\n",
       "      <th>lags_1</th>\n",
       "      <th>lags_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>2004-10-29 00:00:00</td>\n",
       "      <td>-0.105342</td>\n",
       "      <td>-0.080372</td>\n",
       "      <td>-0.156780</td>\n",
       "      <td>-0.092646</td>\n",
       "      <td>0.047289</td>\n",
       "      <td>-0.055106</td>\n",
       "      <td>-0.221166</td>\n",
       "      <td>-0.521421</td>\n",
       "      <td>-0.948255</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030725</td>\n",
       "      <td>-0.076813</td>\n",
       "      <td>-0.122441</td>\n",
       "      <td>-0.170407</td>\n",
       "      <td>-0.097404</td>\n",
       "      <td>0.115540</td>\n",
       "      <td>0.094026</td>\n",
       "      <td>-0.011523</td>\n",
       "      <td>-0.397106</td>\n",
       "      <td>-0.826436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511</th>\n",
       "      <td>2004-10-29 01:00:00</td>\n",
       "      <td>-0.080372</td>\n",
       "      <td>-0.156780</td>\n",
       "      <td>-0.092646</td>\n",
       "      <td>0.047289</td>\n",
       "      <td>-0.055106</td>\n",
       "      <td>-0.221166</td>\n",
       "      <td>-0.521421</td>\n",
       "      <td>-0.948255</td>\n",
       "      <td>-1.362899</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076813</td>\n",
       "      <td>-0.122441</td>\n",
       "      <td>-0.170407</td>\n",
       "      <td>-0.097404</td>\n",
       "      <td>0.115540</td>\n",
       "      <td>0.094026</td>\n",
       "      <td>-0.011523</td>\n",
       "      <td>-0.397106</td>\n",
       "      <td>-0.826436</td>\n",
       "      <td>-1.214351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512</th>\n",
       "      <td>2004-10-29 02:00:00</td>\n",
       "      <td>-0.156780</td>\n",
       "      <td>-0.092646</td>\n",
       "      <td>0.047289</td>\n",
       "      <td>-0.055106</td>\n",
       "      <td>-0.221166</td>\n",
       "      <td>-0.521421</td>\n",
       "      <td>-0.948255</td>\n",
       "      <td>-1.362899</td>\n",
       "      <td>-1.622300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.122441</td>\n",
       "      <td>-0.170407</td>\n",
       "      <td>-0.097404</td>\n",
       "      <td>0.115540</td>\n",
       "      <td>0.094026</td>\n",
       "      <td>-0.011523</td>\n",
       "      <td>-0.397106</td>\n",
       "      <td>-0.826436</td>\n",
       "      <td>-1.214351</td>\n",
       "      <td>-1.449815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513</th>\n",
       "      <td>2004-10-29 03:00:00</td>\n",
       "      <td>-0.092646</td>\n",
       "      <td>0.047289</td>\n",
       "      <td>-0.055106</td>\n",
       "      <td>-0.221166</td>\n",
       "      <td>-0.521421</td>\n",
       "      <td>-0.948255</td>\n",
       "      <td>-1.362899</td>\n",
       "      <td>-1.622300</td>\n",
       "      <td>-1.816869</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.170407</td>\n",
       "      <td>-0.097404</td>\n",
       "      <td>0.115540</td>\n",
       "      <td>0.094026</td>\n",
       "      <td>-0.011523</td>\n",
       "      <td>-0.397106</td>\n",
       "      <td>-0.826436</td>\n",
       "      <td>-1.214351</td>\n",
       "      <td>-1.449815</td>\n",
       "      <td>-1.583501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1514</th>\n",
       "      <td>2004-10-29 04:00:00</td>\n",
       "      <td>0.047289</td>\n",
       "      <td>-0.055106</td>\n",
       "      <td>-0.221166</td>\n",
       "      <td>-0.521421</td>\n",
       "      <td>-0.948255</td>\n",
       "      <td>-1.362899</td>\n",
       "      <td>-1.622300</td>\n",
       "      <td>-1.816869</td>\n",
       "      <td>-1.954480</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097404</td>\n",
       "      <td>0.115540</td>\n",
       "      <td>0.094026</td>\n",
       "      <td>-0.011523</td>\n",
       "      <td>-0.397106</td>\n",
       "      <td>-0.826436</td>\n",
       "      <td>-1.214351</td>\n",
       "      <td>-1.449815</td>\n",
       "      <td>-1.583501</td>\n",
       "      <td>-1.696536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116166</th>\n",
       "      <td>2018-08-01 05:00:00</td>\n",
       "      <td>0.422499</td>\n",
       "      <td>0.326766</td>\n",
       "      <td>-0.031118</td>\n",
       "      <td>-0.492645</td>\n",
       "      <td>-0.857230</td>\n",
       "      <td>-1.177993</td>\n",
       "      <td>-1.316519</td>\n",
       "      <td>-1.431649</td>\n",
       "      <td>-1.351272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.501312</td>\n",
       "      <td>0.458576</td>\n",
       "      <td>0.347138</td>\n",
       "      <td>0.064324</td>\n",
       "      <td>-0.367647</td>\n",
       "      <td>-0.745490</td>\n",
       "      <td>-1.030952</td>\n",
       "      <td>-1.216245</td>\n",
       "      <td>-1.359990</td>\n",
       "      <td>-1.317001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116167</th>\n",
       "      <td>2018-08-01 06:00:00</td>\n",
       "      <td>0.326766</td>\n",
       "      <td>-0.031118</td>\n",
       "      <td>-0.492645</td>\n",
       "      <td>-0.857230</td>\n",
       "      <td>-1.177993</td>\n",
       "      <td>-1.316519</td>\n",
       "      <td>-1.431649</td>\n",
       "      <td>-1.351272</td>\n",
       "      <td>-1.087313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.458576</td>\n",
       "      <td>0.347138</td>\n",
       "      <td>0.064324</td>\n",
       "      <td>-0.367647</td>\n",
       "      <td>-0.745490</td>\n",
       "      <td>-1.030952</td>\n",
       "      <td>-1.216245</td>\n",
       "      <td>-1.359990</td>\n",
       "      <td>-1.317001</td>\n",
       "      <td>-1.118482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116168</th>\n",
       "      <td>2018-08-01 07:00:00</td>\n",
       "      <td>-0.031118</td>\n",
       "      <td>-0.492645</td>\n",
       "      <td>-0.857230</td>\n",
       "      <td>-1.177993</td>\n",
       "      <td>-1.316519</td>\n",
       "      <td>-1.431649</td>\n",
       "      <td>-1.351272</td>\n",
       "      <td>-1.087313</td>\n",
       "      <td>-0.691662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.347138</td>\n",
       "      <td>0.064324</td>\n",
       "      <td>-0.367647</td>\n",
       "      <td>-0.745490</td>\n",
       "      <td>-1.030952</td>\n",
       "      <td>-1.216245</td>\n",
       "      <td>-1.359990</td>\n",
       "      <td>-1.317001</td>\n",
       "      <td>-1.118482</td>\n",
       "      <td>-0.756927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116169</th>\n",
       "      <td>2018-08-01 08:00:00</td>\n",
       "      <td>-0.492645</td>\n",
       "      <td>-0.857230</td>\n",
       "      <td>-1.177993</td>\n",
       "      <td>-1.316519</td>\n",
       "      <td>-1.431649</td>\n",
       "      <td>-1.351272</td>\n",
       "      <td>-1.087313</td>\n",
       "      <td>-0.691662</td>\n",
       "      <td>-0.408344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064324</td>\n",
       "      <td>-0.367647</td>\n",
       "      <td>-0.745490</td>\n",
       "      <td>-1.030952</td>\n",
       "      <td>-1.216245</td>\n",
       "      <td>-1.359990</td>\n",
       "      <td>-1.317001</td>\n",
       "      <td>-1.118482</td>\n",
       "      <td>-0.756927</td>\n",
       "      <td>-0.486737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116170</th>\n",
       "      <td>2018-08-01 09:00:00</td>\n",
       "      <td>-0.857230</td>\n",
       "      <td>-1.177993</td>\n",
       "      <td>-1.316519</td>\n",
       "      <td>-1.431649</td>\n",
       "      <td>-1.351272</td>\n",
       "      <td>-1.087313</td>\n",
       "      <td>-0.691662</td>\n",
       "      <td>-0.408344</td>\n",
       "      <td>-0.200168</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.367647</td>\n",
       "      <td>-0.745490</td>\n",
       "      <td>-1.030952</td>\n",
       "      <td>-1.216245</td>\n",
       "      <td>-1.359990</td>\n",
       "      <td>-1.317001</td>\n",
       "      <td>-1.118482</td>\n",
       "      <td>-0.756927</td>\n",
       "      <td>-0.486737</td>\n",
       "      <td>-0.260521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120563 rows × 697 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date_time   step_16   step_17   step_18   step_19   step_20  \\\n",
       "1558    2004-10-29 00:00:00 -0.105342 -0.080372 -0.156780 -0.092646  0.047289   \n",
       "1511    2004-10-29 01:00:00 -0.080372 -0.156780 -0.092646  0.047289 -0.055106   \n",
       "1512    2004-10-29 02:00:00 -0.156780 -0.092646  0.047289 -0.055106 -0.221166   \n",
       "1513    2004-10-29 03:00:00 -0.092646  0.047289 -0.055106 -0.221166 -0.521421   \n",
       "1514    2004-10-29 04:00:00  0.047289 -0.055106 -0.221166 -0.521421 -0.948255   \n",
       "...                     ...       ...       ...       ...       ...       ...   \n",
       "116166  2018-08-01 05:00:00  0.422499  0.326766 -0.031118 -0.492645 -0.857230   \n",
       "116167  2018-08-01 06:00:00  0.326766 -0.031118 -0.492645 -0.857230 -1.177993   \n",
       "116168  2018-08-01 07:00:00 -0.031118 -0.492645 -0.857230 -1.177993 -1.316519   \n",
       "116169  2018-08-01 08:00:00 -0.492645 -0.857230 -1.177993 -1.316519 -1.431649   \n",
       "116170  2018-08-01 09:00:00 -0.857230 -1.177993 -1.316519 -1.431649 -1.351272   \n",
       "\n",
       "         step_21   step_22   step_23   step_24  ...    lags_9    lags_8  \\\n",
       "1558   -0.055106 -0.221166 -0.521421 -0.948255  ... -0.030725 -0.076813   \n",
       "1511   -0.221166 -0.521421 -0.948255 -1.362899  ... -0.076813 -0.122441   \n",
       "1512   -0.521421 -0.948255 -1.362899 -1.622300  ... -0.122441 -0.170407   \n",
       "1513   -0.948255 -1.362899 -1.622300 -1.816869  ... -0.170407 -0.097404   \n",
       "1514   -1.362899 -1.622300 -1.816869 -1.954480  ... -0.097404  0.115540   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "116166 -1.177993 -1.316519 -1.431649 -1.351272  ...  0.501312  0.458576   \n",
       "116167 -1.316519 -1.431649 -1.351272 -1.087313  ...  0.458576  0.347138   \n",
       "116168 -1.431649 -1.351272 -1.087313 -0.691662  ...  0.347138  0.064324   \n",
       "116169 -1.351272 -1.087313 -0.691662 -0.408344  ...  0.064324 -0.367647   \n",
       "116170 -1.087313 -0.691662 -0.408344 -0.200168  ... -0.367647 -0.745490   \n",
       "\n",
       "          lags_7    lags_6    lags_5    lags_4    lags_3    lags_2    lags_1  \\\n",
       "1558   -0.122441 -0.170407 -0.097404  0.115540  0.094026 -0.011523 -0.397106   \n",
       "1511   -0.170407 -0.097404  0.115540  0.094026 -0.011523 -0.397106 -0.826436   \n",
       "1512   -0.097404  0.115540  0.094026 -0.011523 -0.397106 -0.826436 -1.214351   \n",
       "1513    0.115540  0.094026 -0.011523 -0.397106 -0.826436 -1.214351 -1.449815   \n",
       "1514    0.094026 -0.011523 -0.397106 -0.826436 -1.214351 -1.449815 -1.583501   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "116166  0.347138  0.064324 -0.367647 -0.745490 -1.030952 -1.216245 -1.359990   \n",
       "116167  0.064324 -0.367647 -0.745490 -1.030952 -1.216245 -1.359990 -1.317001   \n",
       "116168 -0.367647 -0.745490 -1.030952 -1.216245 -1.359990 -1.317001 -1.118482   \n",
       "116169 -0.745490 -1.030952 -1.216245 -1.359990 -1.317001 -1.118482 -0.756927   \n",
       "116170 -1.030952 -1.216245 -1.359990 -1.317001 -1.118482 -0.756927 -0.486737   \n",
       "\n",
       "          lags_0  \n",
       "1558   -0.826436  \n",
       "1511   -1.214351  \n",
       "1512   -1.449815  \n",
       "1513   -1.583501  \n",
       "1514   -1.696536  \n",
       "...          ...  \n",
       "116166 -1.317001  \n",
       "116167 -1.118482  \n",
       "116168 -0.756927  \n",
       "116169 -0.486737  \n",
       "116170 -0.260521  \n",
       "\n",
       "[120563 rows x 697 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed=preparion_train (df,24*7*4,'scaled_aep_mw')\n",
    "df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=df_processed.loc[df_processed['date_time']<='2017-08-02 23:00:00',df_processed.columns[df_processed.columns.str.contains('lags_')]]\n",
    "X_test=df_processed.loc[(df_processed['date_time']>='2017-08-02 08:00:00') &\\\n",
    "    (pd.to_datetime(df_processed['date_time']).dt.hour==8),df_processed.columns[df_processed.columns.str.contains('lags_')]]\n",
    "X_train=np.array(X_train).reshape(X_train.shape[0],1,X_train.shape[1])\n",
    "X_test=np.array(X_test).reshape(X_test.shape[0],1,X_test.shape[1])\n",
    "\n",
    "y_train=df_processed.loc[df_processed['date_time']<='2017-08-02 23:00:00',df_processed.columns[df_processed.columns.str.contains('step_')]]\n",
    "y_test=df_processed.loc[(df_processed['date_time']>='2017-08-02 08:00:00') &\\\n",
    "    (pd.to_datetime(df_processed['date_time']).dt.hour==8),df_processed.columns[df_processed.columns.str.contains('step_')]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense,Input,LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_fit,X_val,y_fit,y_val=train_test_split(X_train,y_train,test_size=0.3,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoppage_rule=EarlyStopping(patience=20,monitor='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_fun(X_fit,y_fit,X_val,y_val,num_layers,num_nerons,alpha,iter,b_s):\n",
    "\n",
    "    # Input layer\n",
    "    model=Sequential()\n",
    "    #model.add(Input(X_fit.shape[0],X_fit.shape[1],X_fit.shape[2]))\n",
    "    # Hidden Layers\n",
    "    for i in range(num_layers):\n",
    "        if i==0:\n",
    "            model.add(LSTM(units=num_nerons,return_sequences=True,input_shape=(X_fit.shape[1],X_fit.shape[2])))\n",
    "        elif i<num_layers-1:\n",
    "            model.add(LSTM(units=num_nerons,return_sequences=True))\n",
    "        else :\n",
    "            model.add(LSTM(units=num_nerons))\n",
    "    # output layer\n",
    "    model.add(Dense(units=24))\n",
    "    # Training\n",
    "    model.compile(optimizer=Adam(learning_rate=alpha),loss='mse')\n",
    "    model.fit(X_fit,y_fit,validation_data=(X_val,y_val),epochs=iter,batch_size=b_s,callbacks=[stoppage_rule])\n",
    "    # Evaluation of the model\n",
    "    pred=model.predict(X_val)\n",
    "    y_val2=scaler.inverse_transform(np.array(y_val).reshape(-1,1))\n",
    "    pred2=scaler.inverse_transform(pred.reshape(-1,1))\n",
    "    error=mean_absolute_percentage_error(y_val2,pred2)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-17 12:45:55,296] A new study created in memory with name: no-name-844711af-0050-4a80-a164-f029edd32755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "Epoch 1/1000\n",
      "Epoch 1/1000\n",
      "Epoch 1/1000\n",
      "Epoch 1/1000\n",
      "Epoch 1/1000\n",
      "Epoch 1/1000\n",
      "Epoch 1/1000\n",
      "83/83 [==============================] - 48s 318ms/step - loss: 0.9606 - val_loss: 1.1719\n",
      "261/580 [============>.................] - ETA: 32s - loss: 0.9714Epoch 2/1000\n",
      "83/83 [==============================] - 17s 202ms/step - loss: 0.9512 - val_loss: 1.1563\n",
      "397/580 [===================>..........] - ETA: 19s - loss: 0.9629Epoch 3/1000\n",
      "83/83 [==============================] - 19s 234ms/step - loss: 0.9511 - val_loss: 1.1693\n",
      "101/144 [====================>.........] - ETA: 7s - loss: 0.2421Epoch 4/1000\n",
      "83/83 [==============================] - 19s 225ms/step - loss: 0.9511 - val_loss: 1.1536\n",
      "Epoch 5/1000\n",
      "141/141 [==============================] - 119s 399ms/step - loss: 0.2557 - val_loss: 0.2897\n",
      "Epoch 2/1000\n",
      "144/144 [==============================] - 120s 395ms/step - loss: 0.2195 - val_loss: 0.2463\n",
      "Epoch 2/1000\n",
      "85/85 [==============================] - 105s 372ms/step - loss: 0.2482 - val_loss: 0.2725\n",
      "Epoch 2/1000\n",
      "83/83 [==============================] - 28s 343ms/step - loss: 0.9509 - val_loss: 1.1543\n",
      "Epoch 6/1000\n",
      "580/580 [==============================] - 144s 210ms/step - loss: 0.9599 - val_loss: 1.1815\n",
      "Epoch 2/1000\n",
      "85/85 [==============================] - 28s 334ms/step - loss: 0.1727 - val_loss: 0.2643\n",
      "Epoch 3/1000\n",
      "83/83 [==============================] - 149s 486ms/step - loss: 0.3240 - val_loss: 0.3074\n",
      " 50/580 [=>............................] - ETA: 2:14 - loss: 0.9614Epoch 2/1000\n",
      "83/83 [==============================] - 34s 409ms/step - loss: 0.9509 - val_loss: 1.1549\n",
      "37/83 [============>.................] - ETA: 11s - loss: 0.2230Epoch 7/1000\n",
      "141/141 [==============================] - 43s 309ms/step - loss: 0.2016 - val_loss: 0.2711\n",
      "Epoch 3/1000\n",
      "144/144 [==============================] - 62s 430ms/step - loss: 0.1461 - val_loss: 0.2316\n",
      "Epoch 3/1000\n",
      "85/85 [==============================] - 35s 406ms/step - loss: 0.1560 - val_loss: 0.2588\n",
      "Epoch 4/1000\n",
      "83/83 [==============================] - 32s 388ms/step - loss: 0.2181 - val_loss: 0.2906\n",
      "Epoch 3/1000\n",
      "83/83 [==============================] - 29s 355ms/step - loss: 0.9508 - val_loss: 1.1746\n",
      "Epoch 8/1000\n",
      "141/141 [==============================] - 197s 551ms/step - loss: 0.3047 - val_loss: 0.3191\n",
      "Epoch 2/1000\n",
      "85/85 [==============================] - 29s 338ms/step - loss: 0.1420 - val_loss: 0.2567\n",
      "Epoch 5/1000\n",
      "83/83 [==============================] - 30s 368ms/step - loss: 0.1990 - val_loss: 0.2750\n",
      "Epoch 4/1000\n",
      "141/141 [==============================] - 55s 393ms/step - loss: 0.1852 - val_loss: 0.2755\n",
      "Epoch 4/1000\n",
      "83/83 [==============================] - 31s 369ms/step - loss: 0.9510 - val_loss: 1.1397\n",
      "Epoch 9/1000\n",
      "144/144 [==============================] - 55s 386ms/step - loss: 0.1250 - val_loss: 0.2356\n",
      "Epoch 4/1000\n",
      "85/85 [==============================] - 33s 391ms/step - loss: 0.1324 - val_loss: 0.2513\n",
      "83/83 [==============================] - 32s 389ms/step - loss: 0.1889 - val_loss: 0.2713\n",
      "Epoch 5/1000\n",
      "83/83 [==============================] - 32s 391ms/step - loss: 0.9508 - val_loss: 1.1586\n",
      "  23/1049 [..............................] - ETA: 2:11oss: 0.17Epoch 10/1000\n",
      "141/141 [==============================] - 61s 433ms/step - loss: 0.2159 - val_loss: 0.2927\n",
      "Epoch 3/1000\n",
      "141/141 [==============================] - 52s 371ms/step - loss: 0.1745 - val_loss: 0.2653\n",
      "Epoch 5/1000\n",
      "83/83 [==============================] - 26s 316ms/step - loss: 0.1807 - val_loss: 0.2698\n",
      "Epoch 6/1000\n",
      "83/83 [==============================] - 25s 303ms/step - loss: 0.9506 - val_loss: 1.1676\n",
      "Epoch 11/1000\n",
      "144/144 [==============================] - 47s 324ms/step - loss: 0.1076 - val_loss: 0.2409\n",
      "Epoch 5/1000\n",
      "83/83 [==============================] - 29s 353ms/step - loss: 0.1758 - val_loss: 0.2627\n",
      "Epoch 7/1000\n",
      "83/83 [==============================] - 29s 349ms/step - loss: 0.9509 - val_loss: 1.1719\n",
      "Epoch 12/1000\n",
      "141/141 [==============================] - 49s 348ms/step - loss: 0.1650 - val_loss: 0.2637\n",
      "Epoch 6/1000\n",
      "141/141 [==============================] - 61s 437ms/step - loss: 0.1981 - val_loss: 0.2928\n",
      "Epoch 4/1000\n",
      "580/580 [==============================] - 187s 323ms/step - loss: 0.9527 - val_loss: 1.1775\n",
      " 39/141 [=======>......................] - ETA: 20s - loss: 0.1622Epoch 3/1000\n",
      "144/144 [==============================] - 45s 311ms/step - loss: 0.0953 - val_loss: 0.2387\n",
      "Epoch 6/1000\n",
      "83/83 [==============================] - 26s 317ms/step - loss: 0.1725 - val_loss: 0.2756\n",
      "Epoch 8/1000\n",
      "83/83 [==============================] - 25s 308ms/step - loss: 0.9508 - val_loss: 1.1718\n",
      "Epoch 13/1000\n",
      "712/712 [==============================] - 344s 332ms/step - loss: 0.3457 - val_loss: 0.4169\n",
      "Epoch 2/1000\n",
      "83/83 [==============================] - 33s 404ms/step - loss: 0.1696 - val_loss: 0.2677\n",
      "Epoch 9/1000\n",
      "83/83 [==============================] - 31s 374ms/step - loss: 0.9508 - val_loss: 1.1550\n",
      "Epoch 14/1000\n",
      "141/141 [==============================] - 49s 350ms/step - loss: 0.1598 - val_loss: 0.2700\n",
      "Epoch 7/1000\n",
      "144/144 [==============================] - 55s 384ms/step - loss: 0.0847 - val_loss: 0.2460\n",
      " 786/1049 [=====================>........] - ETA: 44sEpoch 7/1000\n",
      "141/141 [==============================] - 64s 459ms/step - loss: 0.1858 - val_loss: 0.2795\n",
      "Epoch 5/1000\n",
      "83/83 [==============================] - 31s 371ms/step - loss: 0.1662 - val_loss: 0.2612\n",
      "Epoch 10/1000\n",
      "83/83 [==============================] - 30s 356ms/step - loss: 0.9510 - val_loss: 1.1697\n",
      "Epoch 15/1000\n",
      "141/141 [==============================] - 53s 377ms/step - loss: 0.1538 - val_loss: 0.2609\n",
      " 996/1049 [===========================>..] - ETA: 9sEpoch 8/1000\n",
      "83/83 [==============================] - 30s 366ms/step - loss: 0.9508 - val_loss: 1.1604\n",
      "Epoch 16/1000\n",
      "83/83 [==============================] - 32s 384ms/step - loss: 0.1650 - val_loss: 0.2616\n",
      "Epoch 11/1000\n",
      "1049/1049 [==============================] - 185s 172ms/step0.18\n",
      "144/144 [==============================] - 54s 374ms/step - loss: 0.0775 - val_loss: 0.2586\n",
      "Epoch 8/1000\n",
      "83/83 [==============================] - 28s 345ms/step - loss: 0.9508 - val_loss: 1.1696\n",
      "Epoch 17/1000\n",
      "83/83 [==============================] - 28s 342ms/step - loss: 0.1610 - val_loss: 0.2592\n",
      "Epoch 12/1000\n",
      " 1/83 [..............................] - ETA: 29s - loss: 0.9512194"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-17 12:53:37,752] Trial 2 finished with value: 0.0654591129690065 and parameters: {'num_layers': 2, 'num_nerons': 192, 'alpha': 0.013101613178567003, 'iter': 1000, 'b_s': 922}. Best is trial 2 with value: 0.0654591129690065.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 67s 479ms/step - loss: 0.1807 - val_loss: 0.2792\n",
      "Epoch 6/1000\n",
      "477/712 [===================>..........] - ETA: 57s - loss: 0.3198Epoch 1/1000\n",
      "141/141 [==============================] - 51s 361ms/step - loss: 0.1498 - val_loss: 0.2666\n",
      "122/144 [========================>.....] - ETA: 6s - loss: 0.0692Epoch 9/1000\n",
      "83/83 [==============================] - 28s 335ms/step - loss: 0.1581 - val_loss: 0.2577\n",
      " 37/141 [======>.......................] - ETA: 24s - loss: 0.1466Epoch 13/1000\n",
      "83/83 [==============================] - 28s 338ms/step - loss: 0.9508 - val_loss: 1.1592\n",
      "Epoch 18/1000\n",
      "144/144 [==============================] - 53s 365ms/step - loss: 0.0693 - val_loss: 0.2499\n",
      "Epoch 9/1000\n",
      "83/83 [==============================] - 28s 336ms/step - loss: 0.9509 - val_loss: 1.1832\n",
      " 69/144 [=============>................] - ETA: 20s - loss: 0.0661Epoch 19/1000\n",
      "83/83 [==============================] - 31s 379ms/step - loss: 0.1577 - val_loss: 0.2532\n",
      "Epoch 14/1000\n",
      "580/580 [==============================] - 185s 320ms/step - loss: 0.9533 - val_loss: 1.1455\n",
      "Epoch 4/1000\n",
      "141/141 [==============================] - 68s 483ms/step - loss: 0.1764 - val_loss: 0.2828\n",
      "Epoch 7/1000\n",
      "141/141 [==============================] - 56s 401ms/step - loss: 0.1466 - val_loss: 0.2613\n",
      "Epoch 10/1000\n",
      "83/83 [==============================] - 26s 313ms/step - loss: 0.9510 - val_loss: 1.1674\n",
      "Epoch 20/1000\n",
      "144/144 [==============================] - 52s 357ms/step - loss: 0.0649 - val_loss: 0.2513\n",
      "Epoch 10/1000\n",
      "83/83 [==============================] - 34s 418ms/step - loss: 0.1550 - val_loss: 0.2630\n",
      "Epoch 15/1000\n",
      "83/83 [==============================] - 37s 451ms/step - loss: 0.9510 - val_loss: 1.1699\n",
      "Epoch 21/1000\n",
      "83/83 [==============================] - 38s 455ms/step - loss: 0.1548 - val_loss: 0.2640\n",
      "Epoch 16/1000\n",
      "141/141 [==============================] - 57s 404ms/step - loss: 0.1430 - val_loss: 0.2627\n",
      "Epoch 11/1000\n",
      "141/141 [==============================] - 67s 478ms/step - loss: 0.1693 - val_loss: 0.2814\n",
      "Epoch 8/1000\n",
      "144/144 [==============================] - 53s 369ms/step - loss: 0.0600 - val_loss: 0.2522\n",
      " 42/141 [=======>......................] - ETA: 22s - loss: 0.1405Epoch 11/1000\n",
      "83/83 [==============================] - 29s 346ms/step - loss: 0.9508 - val_loss: 1.1633\n",
      "Epoch 22/1000\n",
      "712/712 [==============================] - 255s 358ms/step - loss: 0.3210 - val_loss: 0.4090\n",
      "Epoch 3/1000\n",
      " 22/712 [..............................] - ETA: 4:28 - loss: 0.3211"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    num_layers=trial.suggest_int('num_layers',1,4)\n",
    "    num_nerons=trial.suggest_int('num_nerons',32,256,32)\n",
    "    alpha=trial.suggest_float('alpha',0.001,0.1)\n",
    "    iter=trial.suggest_int('iter',1000,1000)\n",
    "    b_s=trial.suggest_int('b_s',32,1024)\n",
    "    loss_fun=val_fun(X_fit,y_fit,X_val,y_val,num_layers,num_nerons,alpha,iter,b_s)\n",
    "    return loss_fun \n",
    "study = optuna.create_study()\n",
    "study.optimize(objective,n_trials=50,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aep_hourly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
